# 第五章 系統架構設計

本章詳細描述基於激勵相容機制的區塊鏈聯邦學習系統架構。針對第四章提出的委員會捕獲攻擊威脅，本研究提出一個創新的防禦框架，該框架通過樂觀挑戰機制和內部懲罰獎勵系統，在不犧牲系統效率的前提下提供強安全性保證。

## 5.1 系統架構概覽

本節介紹系統的整體架構和核心設計理念。

### 5.1.1 核心角色定義

本系統包含四個核心角色，各自承擔不同的職責：

訓練者（Trainer）：持有本地資料的參與節點，負責在本地資料集上訓練模型並產生本地更新。訓練者不直接參與共識過程，而是將訓練結果提交給聚合者。

聚合者（Aggregator）：負責收集多個訓練者的本地更新，執行聚合演算法（如聯邦平均或Krum），並產生候選全局更新。聚合者需要質押一定數量的代幣以確保其行為誠實。

驗證委員會（Verifier Committee）：由質押權重選出的小型委員會，負責對聚合者提交的更新進行快速驗證和共識。委員會採用拜占庭容錯共識協議達成一致。

挑戰者（Challenger）：任何持有足夠質押的節點都可以擔任挑戰者角色。挑戰者監控已確認的更新，當發現可疑更新時可以提出挑戰，觸發全網驗證。

### 5.1.2 基本工作流程

系統的基本工作流程分為四個階段：

階段一：訓練與聚合。訓練者在本地資料上訓練模型，產生本地更新並提交給聚合者。聚合者收集足夠數量的本地更新後，執行聚合演算法產生候選全局更新。

階段二：樂觀驗證。驗證委員會對候選更新進行快速驗證。驗證過程採用樂觀執行策略，即預設更新有效，委員會主要驗證聚合過程的形式正確性和基本合理性。驗證通過後，更新被標記為「暫時確認」狀態。

階段三：挑戰窗口。暫時確認的更新進入挑戰窗口期（例如10個區塊）。在此期間，任何節點都可以提出挑戰。若挑戰窗口期結束仍無挑戰，更新被標記為「最終確認」狀態。

階段四：挑戰驗證（條件觸發）。若有節點提出挑戰，系統觸發全網驗證流程。所有驗證節點重新執行聚合演算法，將結果與原始更新比對。若發現不一致，原驗證委員會被懲罰，挑戰者獲得獎勵；若一致，挑戰者損失挑戰質押。

### 5.1.3 設計理念

本系統的設計基於以下三個核心理念：

樂觀執行優先。在大多數情況下，參與者是誠實的。因此系統採用樂觀執行策略，預設更新有效，不阻塞訓練流程。這確保了系統在正常情況下的高效運行。

事後問責機制。安全性不依賴於事前的多數誠實假設，而是通過事後的挑戰和懲罰機制來保證。任何惡意行為都可以在事後被檢測和懲罰，這使得系統能夠容忍更高比例的惡意節點。

激勵相容設計。通過精心設計的經濟激勵，使得理性節點的最優策略是誠實行為。攻擊的預期收益為負，因此理性攻擊者不會發動攻擊。這將攻擊者的貪婪轉化為系統的防禦力量。

### 5.1.4 與傳統方法的對比

傳統的區塊鏈聯邦學習系統（如BlockDFL）主要依賴「誠實多數」假設來保證安全性。這種方法存在兩個關鍵問題：首先，為了抵抗共謀攻擊，必須維持較大的委員會規模，這導致通訊複雜度為O(C²)，其中C是委員會大小。其次，當惡意節點通過策略性飢餓攻擊逐步增加質押權重時，系統無法有效防禦。

相較之下，本系統將安全性和活性（liveness）解耦。活性由小型委員會保證，通訊複雜度維持在O(C²small)的低水平。安全性則由挑戰機制和經濟懲罰保證，不依賴於委員會規模。這種設計實現了效率與安全性的雙贏。

## 5.2 樂觀挑戰機制

本節詳細說明樂觀執行和挑戰機制的設計。

### 5.2.1 樂觀執行流程

樂觀執行的核心思想是「先執行，後驗證」。具體流程如下：

當聚合者提交候選更新時，驗證委員會不需要重新執行完整的聚合運算，而是進行輕量級驗證。驗證內容包括：聚合者是否收集了足夠數量的本地更新、更新格式是否正確、更新數值是否在合理範圍內等。

這種輕量級驗證的運算複雜度遠低於完整的聚合運算。例如，若使用Krum聚合演算法，完整執行需要運算所有更新之間的成對距離，複雜度為O(n²)，其中n是訓練者數量。而輕量級驗證只需檢查格式和基本統計特性，複雜度為O(n)。

驗證通過後，更新被寫入區塊鏈並標記為「暫時確認」狀態。此時，訓練者可以立即使用該更新繼續下一輪訓練，不需要等待挑戰窗口期結束。這確保了訓練流程不被阻塞。

### 5.2.2 挑戰者角色與動機

任何持有足夠質押的節點都可以擔任挑戰者。挑戰者的主要動機有兩種：

經濟動機：成功的挑戰可以獲得來自惡意委員會的懲罰獎勵。這種「賞金獵人」機制吸引理性節點主動監控系統安全。

防禦動機：被策略性飢餓攻擊的誠實節點有強烈動機提出挑戰。當誠實節點發現自己長期未被選入委員會或未獲得獎勵時，可以通過挑戰機制揭露惡意委員會的不當行為，恢復公平的獎勵分配。

這種雙重動機確保了系統中始終存在足夠的監督力量。即使大部分節點是被動的，只要存在少數主動的挑戰者，系統安全性就能得到保證。

### 5.2.3 挑戰流程設計

挑戰流程包含以下步驟：

步驟一：挑戰提交。挑戰者選定一個處於挑戰窗口期的更新，提交挑戰交易。挑戰交易必須包含挑戰質押，質押金額需足以支付全網驗證的運算成本。這個設計防止了惡意挑戰者通過大量無效挑戰來癱瘓系統。

步驟二：全網驗證觸發。智能合約接收到挑戰交易後，觸發全網驗證流程。所有驗證節點（或隨機選出的大型驗證委員會）被要求重新執行聚合演算法。

步驟三：驗證執行。驗證節點從區塊鏈上讀取原始的本地更新資料，在本地重新執行聚合演算法（如Krum），產生驗證結果。

步驟四：結果比對與判定。智能合約收集驗證節點的結果，採用多數投票機制達成最終判定。若多數驗證節點的結果與原始更新不一致，則判定挑戰成功；否則判定挑戰失敗。

步驟五：獎懲執行。若挑戰成功，原驗證委員會的質押被罰沒（slashing），其中一部分分配給挑戰者作為獎勵，一部分分配給參與驗證的節點作為運算補償，剩餘部分銷毀。若挑戰失敗，挑戰者的挑戰質押被罰沒，用於補償參與驗證的節點。

### 5.2.4 挑戰窗口期設計

挑戰窗口期的長度需要在兩個目標之間平衡：

足夠長以允許挑戰。挑戰者需要時間來檢測可疑更新、準備挑戰證明、提交挑戰交易。若窗口期過短，誠實挑戰者可能來不及反應。

足夠短以保證最終性。訓練流程需要確定性的全局更新才能繼續。若窗口期過長，會延遲訓練進度。

基於實際的區塊鏈出塊時間和運算能力，建議挑戰窗口期設置為10-20個區塊。以以太坊為例，出塊時間約12秒，則窗口期為2-4分鐘。這個時間足以讓自動化的監控程序檢測異常並提交挑戰。

### 5.2.5 與傳統同步驗證的對比

傳統的同步驗證方法要求所有驗證節點在每次更新時都執行完整的聚合運算。這種方法的優點是安全性高，但存在嚴重的效率問題：

運算冗餘：若有C個驗證節點，則相同的聚合運算被重複執行C次，造成C倍的運算資源浪費。

通訊開銷：所有驗證節點都需要接收完整的本地更新資料，通訊複雜度為O(C·n)，其中n是訓練者數量。

延遲增加：必須等待所有驗證節點完成運算並達成共識後，訓練才能繼續，這增加了每輪訓練的延遲。

相較之下，樂觀挑戰機制在正常情況下（無挑戰發生）只需要輕量級驗證，運算和通訊開銷都大幅降低。只有在異常情況下（挑戰發生）才觸發完整驗證，而這種情況的發生概率p通常很小（在激勵相容的設計下，理性攻擊者不會發動攻擊）。因此，系統的平均運算開銷為O(C·n·p)，遠低於傳統方法的O(C·n)。

## 5.3 激勵機制設計

本節說明如何通過經濟激勵確保系統安全性。

### 5.3.1 內部懸賞系統

本系統的核心創新在於「內部懸賞」機制。與傳統的外部激勵（如通貨膨脹或基金會補貼）不同，本系統的獎勵完全來自於惡意行為者的懲罰。

懲罰機制（Slashing）：當驗證委員會被證明提交了錯誤的更新時，其質押的代幣被罰沒。罰沒比例設置為質押金額的較大比例（如50%-100%），以產生足夠的威慑力。

獎勵分配：被罰沒的代幣按以下比例分配：
- 挑戰者獎勵：40%，作為成功挑戰的賞金
- 驗證者補償：30%，分配給參與全網驗證的節點，補償其運算成本
- 系統銷毀：30%，永久銷毀以減少代幣總供應量

這種設計的關鍵優勢在於自我維持性。系統不需要外部資金來源，懲罰本身產生了足夠的獎勵來激勵監督行為。這使得系統在長期運行中保持激勵的可持續性。

### 5.3.2 激勵相容性分析

激勵相容性是指系統設計使得參與者的最優策略與系統目標一致。本系統通過以下分析確保激勵相容：

理性攻擊者的決策模型：假設攻擊者是理性的經濟人，追求預期收益最大化。攻擊者在決定是否發動攻擊時，會比較攻擊的預期收益與預期成本。

攻擊的潛在收益Gattack包括：
- 通過提交次優更新獲得的獨佔獎勵
- 通過操縱模型可能獲得的外部收益（如市場操縱）

攻擊的預期成本為Pcatch × Lslash，其中Pcatch是被抓到的概率，Lslash是懲罰金額。

在本系統的「1-of-N」挑戰模型下，只要存在至少一個誠實且有能力的挑戰者，Pcatch ≈ 1。這是因為挑戰者可以獨立驗證更新的正確性，不需要依賴其他節點。

因此，理性攻擊者的預期收益為：
Expected Payoff = Gattack - Pcatch × Lslash ≈ Gattack - Lslash

只要設置Lslash >> Gattack，預期收益為負，理性攻擊者就不會發動攻擊。

### 5.3.3 參數設計

為了實現激勵相容，系統參數需要滿足以下約束：

Slashing比例：設驗證委員會每個成員的質押金額為S，委員會大小為C。若攻擊成功，委員會可能獲得的獎勵約為R（一輪訓練的總獎勵）。為了使攻擊無利可圖，需要：
Slashing Amount = α × S × C >> R
其中α是slashing比例。合理的設置是α = 0.5，S = 10R/C，則Slashing Amount = 5R >> R。

挑戰質押要求：挑戰者需要質押的金額應足以支付全網驗證的成本，但不應過高以免阻礙誠實挑戰。設全網驗證需要N個驗證節點，每個節點的運算成本為c，則挑戰質押應設為：
Challenge Stake = N × c × (1 + β)
其中β是安全邊際（如0.2）。若挑戰失敗，該質押被罰沒並用於補償驗證節點。

最小質押要求：為了防止女巫攻擊（Sybil Attack），系統要求所有參與者（聚合者、驗證者、挑戰者）都必須質押最小金額。該金額應足夠高以使得創建大量女巫身份的成本超過潛在收益。

### 5.3.4 與傳統誠實多數假設的對比

傳統的區塊鏈聯邦學習系統依賴「誠實多數」假設，即假設驗證委員會中超過2/3的成員是誠實的。這種假設存在兩個問題：

靜態假設的脆弱性：誠實多數假設是靜態的，無法應對動態的攻擊策略。如第四章所述的策略性飢餓攻擊，可以使惡意節點的質押權重逐步增長，最終突破誠實多數的閾值。

缺乏經濟理性基礎：誠實多數假設沒有考慮節點的經濟動機。在現實中，節點可能因為經濟利益而選擇共謀，特別是當攻擊收益遠超過誠實行為的收益時。

相較之下，本系統基於「理性節點」假設，這是一個更弱且更現實的假設。系統不要求節點天生誠實，而是通過經濟激勵使得誠實行為成為理性選擇。這種「Security by Deterrence」（威懾安全）的典範比「Security by Majority」（多數安全）更加穩健。

## 5.4 安全性保證分析

本節從理論角度分析系統的安全性保證。

### 5.4.1 安全性不等式

系統的安全性基於以下核心不等式：

Lslash >> Gattack

其中Lslash是攻擊失敗時的損失（slashing懲罰），Gattack是攻擊成功時的潛在收益。

詳細推導如下。設驗證委員會由C個成員組成，每個成員質押金額為S。若委員會共謀提交錯誤更新，被挑戰成功後，總損失為：

Lslash = α × S × C

其中α是slashing比例（如0.5）。

攻擊的潛在收益包括兩部分：

直接收益：通過獨佔獎勵獲得的額外收益。在正常情況下，獎勵R會分配給所有誠實參與者。若惡意委員會排除誠實節點，獨佔獎勵，則額外收益約為R。

間接收益：通過操縱模型可能獲得的外部收益，記為Gext。這取決於具體應用場景，可能包括市場操縱、競爭優勢等。

因此，總收益為：
Gattack = R + Gext

為了確保安全性，需要：
α × S × C >> R + Gext

在實際設計中，可以設置S = 10R/C，α = 0.5，則：
Lslash = 0.5 × 10R/C × C = 5R

若假設Gext ≤ 4R（這對大多數應用是合理的），則Lslash = 5R > Gattack = R + Gext，滿足安全性條件。

### 5.4.2 1-of-N安全模型

本系統的安全性基於「1-of-N」模型，即只要N個節點中存在至少1個誠實且有能力的挑戰者，系統就是安全的。

這個模型的關鍵在於挑戰的獨立性。與傳統的多數投票不同，挑戰者不需要與其他節點協調或達成共識。任何單個挑戰者都可以獨立驗證更新的正確性，並提出挑戰。

因此，被抓到的概率Pcatch可以表示為：
Pcatch = 1 - (1 - phon)^N

其中phon是單個節點誠實且有能力的概率，N是潛在挑戰者的數量。

即使phon很小（如0.1），只要N足夠大（如N = 100），Pcatch也接近1：
Pcatch = 1 - (1 - 0.1)^100 = 1 - 0.9^100 ≈ 0.99997

這意味著攻擊幾乎必然被檢測到。在這種情況下，理性攻擊者的預期收益為：
Expected Payoff = Gattack - Pcatch × Lslash ≈ Gattack - Lslash < 0

因此，理性攻擊者不會發動攻擊。

### 5.4.3 信任假設層級提升

根據現有文獻的分析，區塊鏈聯邦學習系統可以按信任假設分為六個層級：

Level 1：完全信任中心化服務器
Level 2：信任聯邦成員誠實
Level 3：信任驗證者誠實多數（>2/3）
Level 4：信任驗證者誠實少數（>1/3）
Level 5：信任驗證者理性（無共謀）
Level 6：信任驗證者理性（允許共謀但有經濟懲罰）

傳統的區塊鏈聯邦學習系統（如BlockDFL）運行在Level 3，即假設驗證委員會中超過2/3的成員是誠實的。這個假設在面對策略性攻擊時是脆弱的。

本系統運行在Level 6，即允許驗證者共謀，但通過經濟懲罰機制使得共謀行為無利可圖。這是一個更弱的信任假設，因為它不要求節點天生誠實，只要求節點是理性的經濟人。

這種信任假設的提升顯著增強了系統的安全性。即使在極端情況下（如所有驗證委員會成員都是惡意的），只要存在至少一個理性的挑戰者，系統仍然是安全的。

### 5.4.4 攻擊成本分析

從攻擊者的角度分析，要成功攻擊本系統需要滿足以下條件：

控制驗證委員會：攻擊者需要在某一輪中控制驗證委員會的多數席位。這需要累積足夠的質押權重，成本為O(S × C)。

阻止所有挑戰者：攻擊者需要確保沒有任何節點提出挑戰。在1-of-N模型下，這需要控制或賄賂所有N個潛在挑戰者，成本為O(N × Bribe)。

即使攻擊成功，收益也只是一輪訓練的獎勵R加上可能的外部收益Gext。而攻擊失敗的損失是Lslash = α × S × C。

因此，攻擊的成本效益比為：
Cost/Benefit = (S × C + N × Bribe) / (R + Gext)

在合理的參數設置下（如S = 10R/C），這個比值遠大於1，使得攻擊在經濟上不可行。

## 5.5 效率分析

本節分析系統的效率特性，並與傳統方法進行對比。

### 5.5.1 安全性與活性解耦

傳統的區塊鏈聯邦學習系統將安全性和活性（liveness）耦合在一起，都依賴於驗證委員會。這導致了一個兩難困境：

若委員會規模C較小（如C = 7），系統具有高效率（低通訊開銷、低延遲），但安全性較弱，容易受到共謀攻擊。

若委員會規模C較大（如C = 100），系統具有高安全性（難以共謀），但效率較低，通訊複雜度為O(C²)。

本系統通過挑戰機制實現了安全性與活性的解耦：

活性保證：由小型驗證委員會（如C = 7）負責快速達成共識，確保訓練流程不被阻塞。委員會的主要職責是保證系統的liveness，而非security。

安全性保證：由挑戰機制和經濟懲罰負責。安全性不依賴於委員會規模，而是依賴於懲罰的威懾力和挑戰者的存在。

這種解耦設計使得系統可以同時實現高效率和高安全性，打破了傳統的效率-安全性權衡。

### 5.5.2 通訊複雜度分析

通訊複雜度是衡量區塊鏈聯邦學習系統效率的關鍵指標。本節分析不同方法的通訊複雜度。

傳統方法（BlockDFL）：
- 委員會內共識：O(C²)，其中C是委員會大小
- 若要提高安全性，需增大C，則複雜度增長為O(C²large)
- 例如，C = 100時，複雜度為O(10,000)

本方法：
- 正常情況（無挑戰）：O(C²small)，其中Csmall是小型委員會大小（如7）
- 挑戰情況：O(N²)，其中N是參與全網驗證的節點數
- 平均複雜度：O(C²small + p × N²)，其中p是挑戰發生概率

關鍵觀察：在激勵相容的設計下，理性攻擊者不會發動攻擊，因此p ≈ 0。即使考慮誤報或非理性行為，p也應該很小（如p < 0.01）。

定量對比：
- BlockDFL（高安全性配置）：O(100²) = O(10,000)
- 本方法：O(7² + 0.01 × 100²) = O(49 + 100) = O(149)
- 效率提升：約67倍

這個巨大的效率提升來自於兩個因素：委員會規模的縮減（100 → 7）和挑戰的低頻率（p ≈ 0.01）。

### 5.5.3 運算開銷分析

運算開銷主要來自於聚合演算法的執行。以Krum演算法為例，其複雜度為O(n²)，其中n是訓練者數量。

傳統同步驗證方法：
- 每個驗證節點都執行完整的Krum演算法
- 總運算量：C × O(n²)
- 例如，C = 100, n = 1000時，總運算量為100 × O(1,000,000) = O(100,000,000)

本方法：
- 正常情況：只有聚合者執行Krum，驗證委員會執行輕量級驗證
- 聚合者運算量：O(n²)
- 驗證委員會運算量：O(n)（僅檢查格式和基本統計）
- 總運算量：O(n²) + C × O(n) ≈ O(n²)
- 挑戰情況：N個驗證節點執行Krum
- 總運算量：N × O(n²)
- 平均運算量：O(n²) + p × N × O(n²) = O((1 + p × N) × n²)

定量對比（C = 100, N = 100, n = 1000, p = 0.01）：
- 傳統方法：100 × O(1,000,000) = O(100,000,000)
- 本方法：(1 + 0.01 × 100) × O(1,000,000) = 2 × O(1,000,000) = O(2,000,000)
- 效率提升：約50倍

### 5.5.4 端到端延遲分析

端到端延遲是指從訓練者提交本地更新到全局更新被最終確認的時間。

傳統方法：
- 聚合時間：Tagg
- 委員會驗證時間：Tverify（包括重新執行聚合）
- 共識時間：Tconsensus
- 總延遲：Tagg + Tverify + Tconsensus

本方法（正常情況）：
- 聚合時間：Tagg
- 輕量級驗證時間：Tlight（遠小於Tverify）
- 共識時間：Tconsensus
- 挑戰窗口期：Twindow（但不阻塞訓練）
- 總延遲（訓練視角）：Tagg + Tlight + Tconsensus
- 總延遲（最終確認視角）：Tagg + Tlight + Tconsensus + Twindow

關鍵觀察：從訓練流程的角度，本方法的延遲為Tagg + Tlight + Tconsensus，由於Tlight << Tverify，這顯著低於傳統方法。雖然最終確認需要額外的Twindow，但這不影響訓練的進行。

### 5.5.5 與BlockDFL的定量對比

表5.1總結了本方法與BlockDFL在各項指標上的對比。

| 指標 | BlockDFL | 本方法 | 改進 |
|------|----------|--------|------|
| 信任假設 | Level 3: 誠實多數 (>2/3) | Level 6: 理性節點 | 更強 |
| 委員會角色 | 安全性 + 活性（耦合） | 活性（解耦） | 更靈活 |
| 委員會規模 | 需增大以提高安全性 | 可保持最小 | 更高效 |
| 通訊複雜度 | O(C²large) | O(C²small) | ~67倍 |
| 運算複雜度 | O(C × n²) | O((1+p×N) × n²) | ~50倍 |
| 共謀抵抗性 | 小委員會時脆弱 | 小委員會時仍強健 | 顯著提升 |
| 訓練延遲 | Tagg + Tverify + Tconsensus | Tagg + Tlight + Tconsensus | 降低 |

這個對比清楚地展示了本方法在保證更高安全性的同時，實現了顯著的效率提升。這種「Security-Efficiency Win-Win」的結果來自於核心設計理念的轉變：從「Security by Majority」到「Security by Deterrence」。

## 5.6 系統實現細節

本節說明關鍵技術細節和實現考量。

### 5.6.1 智能合約架構

系統的核心邏輯通過三個智能合約實現：

質押管理合約（StakeManager）：
- 功能：管理所有參與者的質押、解質押、獎勵分配
- 關鍵方法：
  - stake(amount): 節點質押代幣
  - unstake(amount): 節點解除質押（需等待解鎖期）
  - distributeReward(round, recipients, amounts): 分配獎勵
  - slash(validator, amount): 執行懲罰
- 狀態變量：
  - stakes: mapping(address => uint256)，記錄每個地址的質押金額
  - totalStake: uint256，總質押金額
  - lockPeriod: uint256，解質押鎖定期

更新管理合約（UpdateManager）：
- 功能：管理全局更新的提交、驗證、確認
- 關鍵方法：
  - submitUpdate(round, updateHash, localUpdates): 聚合者提交更新
  - verifyUpdate(round, approved): 驗證委員會投票
  - finalizeUpdate(round): 挑戰窗口期結束後最終確認
- 狀態變量：
  - updates: mapping(uint256 => Update)，記錄每輪的更新資訊
  - Update結構：{updateHash, status, submitter, timestamp, votes}
  - challengeWindow: uint256，挑戰窗口期長度

挑戰管理合約（ChallengeManager）：
- 功能：管理挑戰的提交、驗證、判定
- 關鍵方法：
  - submitChallenge(round, stake): 提交挑戰
  - submitVerification(challengeId, result): 驗證節點提交驗證結果
  - resolveChallenge(challengeId): 判定挑戰結果並執行獎懲
- 狀態變量：
  - challenges: mapping(uint256 => Challenge)
  - Challenge結構：{round, challenger, stake, status, verifications}

### 5.6.2 鏈上與鏈下運算分工

為了平衡安全性和效率，系統採用鏈上鏈下混合架構：

鏈上運算（智能合約執行）：
- 質押和解質押邏輯
- 委員會選擇（基於質押權重的隨機選擇）
- 投票和共識邏輯
- 懲罰和獎勵分配
- 挑戰判定（基於驗證節點的投票）

鏈下運算（節點本地執行）：
- 模型訓練（在本地資料上）
- 聚合演算法執行（如Krum）
- 挑戰驗證（重新執行聚合演算法）
- 更新的儲存和傳輸

這種分工的理由是：鏈上運算需要支付gas費用且受到區塊gas限制，因此只適合執行關鍵的、輕量級的邏輯。運算密集型的任務（如模型訓練和聚合）在鏈下執行，只將結果的雜湊值提交到鏈上，既保證了可驗證性又避免了高昂的gas成本。

### 5.6.3 委員會選擇演算法

委員會選擇採用基於質押權重的可驗證隨機函數（Verifiable Random Function, VRF）：

演算法5.1：委員會選擇

輸入：當前輪次round，委員會大小C，所有節點的質押{(node_i, stake_i)}
輸出：選中的委員會成員{member_1, ..., member_C}

1. seed = VRF(round)  // 使用VRF生成隨機種子
2. totalStake = sum(stake_i)
3. committee = []
4. for j = 1 to C:
5.     r = Random(seed, j) mod totalStake  // 生成[0, totalStake)範圍的隨機數
6.     累積和 = 0
7.     for each node_i:
8.         累積和 += stake_i
9.         if 累積和 > r:
10.            committee.append(node_i)
11.            break
12. return committee

這個演算法確保了選擇的公平性（質押越多，被選中概率越高）和可驗證性（任何人都可以驗證選擇結果的正確性）。

### 5.6.4 挑戰驗證演算法

當挑戰被提交後，系統觸發以下驗證流程：

演算法5.2：挑戰驗證

輸入：挑戰的輪次round，原始更新update_original，本地更新集合{local_1, ..., local_n}
輸出：驗證結果（成功/失敗）

1. // 階段一：重新執行聚合
2. update_recomputed = Krum({local_1, ..., local_n})
3. 
4. // 階段二：比對結果
5. if Distance(update_original, update_recomputed) > threshold:
6.     return "挑戰成功"
7. else:
8.     return "挑戰失敗"
9. 
10. // 階段三：多數投票（由智能合約執行）
11. 收集所有驗證節點的結果
12. if 多數節點返回"挑戰成功":
13.     執行懲罰和獎勵分配
14. else:
15.     罰沒挑戰者質押

這裡的Distance函數運算兩個模型更新之間的歐幾里得距離，threshold是預設的容差閾值（用於處理浮點數運算的微小差異）。

### 5.6.5 資料結構設計

更新記錄格式：每個全局更新在區塊鏈上記錄為：

```
Update {
    round: uint256,              // 輪次編號
    updateHash: bytes32,         // 更新內容的雜湊值
    localUpdateHashes: bytes32[], // 所有本地更新的雜湊值
    aggregator: address,         // 聚合者地址
    committee: address[],        // 驗證委員會成員
    status: enum,                // PENDING, CONFIRMED, CHALLENGED, FINALIZED
    timestamp: uint256,          // 提交時間
    votes: mapping(address => bool) // 委員會投票記錄
}
```

挑戰證明格式：挑戰者提交的挑戰包含：

```
Challenge {
    challengeId: uint256,        // 挑戰編號
    round: uint256,              // 被挑戰的輪次
    challenger: address,         // 挑戰者地址
    stake: uint256,              // 挑戰質押金額
    timestamp: uint256,          // 提交時間
    verifications: mapping(address => bytes32), // 驗證節點的結果
    status: enum                 // PENDING, RESOLVED_SUCCESS, RESOLVED_FAIL
}
```

這些資料結構設計確保了所有關鍵資訊都被記錄在鏈上，實現了完整的可審計性和可驗證性。

## 5.7 本章小結

本章詳細描述了基於激勵相容機制的區塊鏈聯邦學習系統架構。系統的核心創新在於將安全性從「誠實多數假設」轉變為「經濟理性假設」，通過樂觀挑戰機制和內部懲罰獎勵系統實現了「Security by Deterrence」。

主要貢獻包括：

系統架構設計：提出了包含訓練者、聚合者、驗證委員會和挑戰者的四角色架構，實現了安全性與活性的解耦。

樂觀挑戰機制：設計了「先執行，後驗證」的流程，在正常情況下保持高效率，在異常情況下觸發全網驗證。

激勵相容機制：通過內部懸賞系統，將攻擊者的懲罰轉化為挑戰者的獎勵，實現了自我維持的安全保證。

安全性分析：證明了在Lslash >> Gattack的條件下，理性攻擊者不會發動攻擊，系統在1-of-N模型下是安全的。

效率分析：展示了系統在通訊複雜度、運算開銷和延遲方面相比傳統方法的顯著優勢，實現了約50-67倍的效率提升。

下一章將通過實驗評估驗證本章提出的理論分析，展示系統在實際場景中的性能表現。
