\begin{ZhChapter}

\chapter{背景知識與相關研究}
\label{chap:background-related}

本章旨在建立理解區塊鏈聯邦學習委員會安全性所需的理論基礎與技術背景。首先，本章將從聯邦學習的核心價值出發，闡明中央化架構面臨的信任困境，進而說明區塊鏈技術如何作為去中心化信任的基礎設施。接著，本章將介紹拜占庭容錯理論的基本原理，為理解委員會共識機制的安全性閾值提供數學基礎。在此基礎上，本章將探討區塊鏈聯邦學習如何從全節點共識演進至委員會架構，並分析委員會規模與安全性之間的權衡關係。隨後，本章將詳細介紹本研究採用的基準系統模型 BlockDFL 之委員會架構，其內容涵蓋角色定義、運作流程與獎勵機制。最後，本章將回顧現有驗證方法的局限性，指出當前研究在面對策略性攻擊者時的盲區，從而定位本研究欲填補的關鍵缺口。

\section{聯邦學習與去中心化信任需求}
\label{sec:fl-trust}

\subsection{聯邦學習的核心機制}
\label{sec:fl-core}

聯邦學習是一種分散式機器學習典範，其核心創新在於實現「資料不動、模型動」的訓練機制 \cite{mcmahan2017communication}。在傳統的集中式機器學習中，所有訓練資料必須彙集至中央伺服器進行處理，這種做法在面對隱私敏資料或資料傳輸成本高昂的場景時顯得力不從心。聯邦學習透過將訓練過程分散至資料所在的終端裝置，僅將模型更新而非原始資料上傳至伺服器進行聚合，從根本上改變了資料與運算的關係。這種架構使得醫療機構能夠在不共享病患紀錄的前提下協同訓練診斷模型，金融機構能夠在不揭露客戶交易資料的情況下建立風險評估系統，行動裝置製造商能夠利用數百萬用戶的使用習慣最佳化輸入法預測，而無需將敏感的打字內容上傳至雲端。

聯邦學習的標準訓練流程可概括為四個階段的迭代循環。在每一輪訓練中，中央伺服器首先將當前的全域模型參數分發給選定的客戶端；各客戶端隨後在本地私有資料上執行若干輪梯度下降，產生反映本地資料特性的模型更新；客戶端將這些更新上傳至伺服器；伺服器執行聚合演算法（最常見的是 FedAvg \cite{mcmahan2017communication}）將各客戶端的更新整合為新的全域模型。此循環持續進行直至模型收斂或達到預設的訓練輪數。值得注意的是，聯邦學習面對的資料分布通常具有高度異質性：不同客戶端持有的資料量可能相差懸殊，資料的類別分布也往往呈現顯著差異，這種非獨立同分布（Non-IID）的特性為模型訓練與安全防護帶來了獨特的挑戰 \cite{kairouz2021advances}。

\subsection{中央化架構的信任困境}
\label{sec:centralized-trust}

儘管聯邦學習在資料隱私保護上取得了重要進展，其標準架構仍存在一個根本性的信任假設：所有參與者必須信任中央聚合伺服器會誠實地執行聚合運算並正確地分發結果。然而，在缺乏有效驗證機制的情況下，這項假設構成了系統的單點脆弱性。中央伺服器可能因遭受攻擊、內部人員惡意行為或系統故障而偏離預期行為，而客戶端對此幾乎無從察覺，更遑論採取補救措施。

中央化架構面臨的信任風險可歸納為三個層面。第一個層面是聚合正確性的不可驗證性。當伺服器宣稱某一全域模型是由特定客戶端更新聚合而成時，客戶端無法獨立驗證此宣稱的真實性。伺服器可能執行選擇性聚合，意即僅納入部分客戶端的更新而排除具體，或者直接篡改聚合結果以植入後門。研究已證實，透過精心設計的模型修改，攻擊者可在不顯著影響主任務效能的情況下，使模型對特定輸入產生預設的錯誤輸出 \cite{bagdasaryan2020how}。第二個層面是單點故障風險。中央伺服器一旦因攻擊、硬體故障或網路問題而離線，整體訓練流程即刻中斷，且由於缺乏分散式的狀態同步機制，系統難以從中間狀態恢復。第三個層面是隱私保護的局限性。儘管聯邦學習避免了原始資料的直接傳輸，研究表明惡意的聚合伺服器仍可能透過分析客戶端提交的模型更新，推論出關於訓練資料的敏感資訊 \cite{zhu2019deep}。

這些信任風險在跨組織協作的場景中尤為突出。當多個相互獨立甚至存在競爭關係的機構希望聯合訓練模型時，由任何單一機構擔任中央聚合者都難以獲得其他參與者的充分信任。即便引入第三方作為中立的聚合服務提供者，仍無法從根本上消除對該第三方誠實性的依賴。這種信任困境限制了聯邦學習在高價值、高敏感場景中的應用潛力，也促使研究者開始探索去中心化的替代方案。

\subsection{區塊鏈作為去中心化信任基礎設施}
\label{sec:blockchain-trust}

區塊鏈技術具備不可篡改性、透明性與去中心化這三項核心特性，恰好對應了中央化聯邦學習面臨的信任困境，使其成為建構去中心化聯邦學習系統的理想基礎設施。不可篡改性源於區塊鏈的鏈式雜湊結構：每個區塊包含前一區塊的雜湊值，任何對歷史資料的修改都將導致後續所有區塊的雜湊值連鎖變化，從而被網路中的其他節點立即偵測。這項特性確保了一旦聚合結果被記錄於區塊鏈，便無法在事後被悄然篡改。透明性則意味著所有被記錄的交易與狀態變更對全體參與者可見，客戶端可以驗證自己的更新是否被納入聚合，也可以審計歷史聚合過程是否遵循預定的規則。去中心化消除了對單一實體的信任依賴：區塊鏈網路由眾多獨立節點共同維護，即使部分節點失效或行為惡意，只要誠實節點佔據多數，系統仍能正確運作。

將區塊鏈整合至聯邦學習架構，可從多個層面強化系統的可信賴性。在聚合正確性方面，智能合約可編碼確定性的聚合規則，確保聚合過程按照預定邏輯執行，而非依賴聚合者的自我約束。聚合結果連同參與者資訊被記錄於區塊鏈，形成永久可查的審計軌跡。在系統可用性方面，區塊鏈的分散式架構天然具備容錯能力：即使部分節點離線，其他節點仍可維持系統運作，避免了中央伺服器故障導致的全面停擺。在激勵對齊方面，區塊鏈原生的代幣機制可用於設計精細的獎懲制度，對誠實貢獻者給予獎勵，對惡意行為者施加經濟懲罰，從而在博弈論意義上引導參與者趨向誠實行為。

然而，區塊鏈並非萬能的信任解決方案。區塊鏈共識機制本身需要假設惡意節點不超過特定比例；以拜占庭容錯協議為例，此閾值通常設定為三分之一。當攻擊者控制的節點超過此閾值時，區塊鏈的安全性保證將不再成立。此外，區塊鏈的共識過程涉及大量的節點間通訊，其延遲與頻寬成本可能與聯邦學習對快速迭代的需求產生張力。這些考量促使研究者發展出委員會架構等效率最佳化方案，但也隨之引入了新的安全性議題。下一節將首先介紹拜占庭容錯的理論基礎，為理解這些安全性議題提供必要的背景知識。

\section{拜占庭容錯的理論基礎}
\label{sec:bft-fundamentals}

\subsection{拜占庭將軍問題與容錯閾值}
\label{sec:byzantine-generals}

\paragraph{拜占庭將軍問題背景}
拜占庭將軍問題由 Lamport、Shostak 與 Pease 於 1982 年正式提出 \cite{lamport1982byzantine}，是分散式系統容錯理論的基石。問題的設定源自一個軍事隱喻：拜占庭帝國的數支軍隊包圍敵城，各軍由一位將軍指揮，將軍們僅能透過信使相互通訊。然而，部分將軍可能是叛徒，他們會刻意傳遞錯誤訊息以阻撓忠誠將軍達成一致決策。問題的核心在於：如何設計一個協議，使得所有忠誠將軍能就「進攻」或「撤退」達成共識，即使存在叛徒試圖破壞協調？此問題的形式化定義包含兩個交互一致性條件：所有忠誠節點必須就相同的值達成共識（一致性），且若發起者是誠實的，則共識結果必須是發起者提出的值（正確性）。

\paragraph{數學限制與 Quorum 交叉原理}
拜占庭將軍問題存在一個根本性的數學限制：在僅使用口頭訊息的情況下，問題可解若且唯若誠實節點超過總數的三分之二。換言之，若系統中有 $n$ 個節點，最多只能容忍 $f$ 個拜占庭節點，其中 $n \geq 3f + 1$。此限制可透過最簡單的三節點、一叛徒場景直觀理解。考慮指揮官向兩位副官發送命令的情境：若指揮官是叛徒，他可能向副官 A 發送「進攻」，向副官 B 發送「撤退」；當兩位誠實副官相互交換收到的命令時，各自都會發現矛盾。然而，若副官 B 是叛徒而指揮官誠實，副官 B 可能向副官 A 謊稱「指揮官說撤退」。關鍵的洞察在於：從副官 A 的視角來看，這兩種情境完全無法區分，因為在兩種情況下，他都收到來自指揮官的「進攻」訊息，以及來自 B 聲稱收到「撤退」的訊息。任何確定性演算法在此情境下都必然失敗，這從根本上限制了拜占庭容錯系統的設計空間。

此三分之一閾值的數學根源在於 Quorum 交叉原理。為確保任何決策都獲得足夠的誠實節點背書，系統需要收集至少 $2f+1$ 個節點的確認。由於最多 $f$ 個節點可能是惡意的，$2f+1$ 個確認中必然包含至少 $f+1$ 個來自誠實節點。任意兩個大小為 $2f+1$ 的節點群體，其交集至少包含 $f+1$ 個節點，這確保了至少有一個誠實節點見證了兩次決策，從而防止系統對同一問題做出矛盾的決定。將節點總數代入約束條件 $n \geq 2f+1+f$，即得 $n \geq 3f+1$。

\subsection{實用拜占庭容錯協議的核心概念}
\label{sec:pbft-core}

拜占庭將軍問題的早期解法雖然在理論上可行，但其指數級的通訊複雜度使其僅具學術意義。1999 年，Castro 與 Liskov 提出實用拜占庭容錯協議（Practical Byzantine Fault Tolerance, PBFT）\cite{castro1999practical}，首次將 BFT 共識的通訊複雜度降至多項式級別 $O(n^2)$，使其在實際系統中可行。PBFT 的設計目標是在部分同步網路模型下，以合理的效能代價換取對任意惡意行為的容錯能力。

PBFT 協議的運作依賴 $n = 3f+1$ 個副本節點，其中一個被指定為主節點（Primary），負責為客戶端請求分配序號並發起共識。協議透過三個階段達成共識：預準備（Pre-prepare）、準備（Prepare）與提交（Commit）。在預準備階段，主節點將請求連同分配的序號廣播給所有副本；在準備階段，收到預準備訊息的副本向其他所有副本廣播準備訊息，當某副本收集到 $2f$ 個匹配的準備訊息時，表明系統中有足夠多的節點認可此請求的序號分配；在提交階段，進入準備狀態的副本廣播提交訊息，當收集到 $2f+1$ 個提交訊息時，副本確信此請求已被系統接受，可以執行並回覆客戶端。三階段設計的核心目的是確保即使主節點是惡意的，也無法導致誠實節點對請求順序產生分歧。

PBFT 的通訊複雜度為 $O(n^2)$，這是因為在準備與提交階段，每個節點都需要向其他所有節點發送訊息。以 $n=7$（可容忍 2 個拜占庭節點）的配置為例，每輪共識約需交換 100 則訊息；當節點數增至 $n=22$（可容忍 7 個拜占庭節點）時，訊息數增至約 900 則。這種二次方增長限制了 PBFT 在大規模網路中的直接應用，實務部署通常限制在 10 至 20 個節點的規模。後續研究如 HotStuff \cite{yin2019hotstuff} 透過流水線化設計與閾值簽章技術，將複雜度進一步降至 $O(n)$，但在本研究關注的許可制聯盟鏈場景中，節點數量通常在 PBFT 可承受的範圍內。

\subsection{BFT 共識在區塊鏈聯邦學習中的角色}
\label{sec:bft-in-bcfl}

在區塊鏈聯邦學習系統中，拜占庭容錯共識扮演著確保聚合結果正確性的關鍵角色。與傳統區塊鏈應用（如加密貨幣交易）不同，聯邦學習的「交易」是模型更新，而「帳本狀態」是全域模型參數。當負責聚合的節點可能被攻陷或本身即為惡意參與者時，系統需要一個機制來驗證聚合結果的正確性，並在多個可能存在分歧的結果中達成共識。BFT 協議正是為此目的而設計：它確保只要惡意節點不超過總數的三分之一，系統就能就聚合結果達成一致，且該結果必然反映誠實多數的判斷。

然而，將 BFT 共識直接應用於大規模聯邦學習系統面臨顯著的效率挑戰。聯邦學習通常涉及數十至數百個參與者，若所有參與者都參與每一輪的 BFT 共識，$O(n^2)$ 的通訊複雜度將成為嚴重的效能瓶頸。更重要的是，聯邦學習需要頻繁迭代（典型的訓練過程可能包含數百至數千輪），因此每輪都執行完整的全網共識將導致訓練時間大幅延長。這種效率與安全性之間的張力，促使研究者發展出委員會架構：由一個小型的代表性子集執行共識，以較低的通訊成本達成近似的安全保證。下一節將詳細探討這種架構演進及其伴隨的安全性權衡。

\section{區塊鏈聯邦學習的委員會架構演進}
\label{sec:committee-evolution}

\subsection{從全節點到委員會的效率驅動}
\label{sec:full-to-committee}

區塊鏈聯邦學習的早期研究嘗試將傳統 BFT 共識直接應用於全體參與者，但很快便遭遇了可擴展性的瓶頸。以 BFLC \cite{li2021blockchain} 的實驗配置為例，當參與者數量達到 20 個時，採用完整 PBFT 共識的每輪延遲已超過 100 毫秒；若將參與者擴展至數百個規模，共識延遲將增長至秒級甚至更長，這對於需要快速迭代的聯邦學習訓練而言顯然無法接受。更根本的問題在於通訊頻寬的消耗：每輪共識中，每個節點都需要接收並處理來自其他所有節點的訊息，當節點數量增加時，網路負擔呈平方級增長，這在頻寬受限的邊緣運算環境中尤為致命。

委員會架構的核心理念是將共識責任委派給一個規模遠小於全網的代表性子集。令全網節點數為 $n$，委員會規模為 $c$，其中 $c \ll n$。委員會內部執行 BFT 共識的通訊成本為 $O(c^2)$，委員會決策結果廣播至全網的成本為 $O(n)$，總通訊成本為 $O(c^2 + n)$。當 $c$ 維持在較小的常數（如 7 至 20）時，此成本近似於線性 $O(n)$，相較於全節點 PBFT 的 $O(n^2)$ 實現了數量級的改善。FLCoin \cite{ren2024scalable} 的實驗資料印證了這一分析：在 500 個節點的網路中，採用規模為 100 的滑動視窗委員會，相較於全節點共識可減少約 90\% 的通訊開銷，共識延遲維持在 3 秒以內。

委員會架構的效率優勢使其迅速成為區塊鏈聯邦學習的主流設計典範。然而，這種效率的提升並非沒有代價：系統的安全性不再由全網的誠實多數保證，而是取決於委員會的組成是否可信。若攻擊者能夠控制委員會中超過三分之一的席位，便可操控共識結果，通過惡意的聚合提案或拒絕誠實的提案。這種從「全網安全」到「委員會安全」的轉變，將安全性分析的焦點從「全網惡意節點比例」轉移至「委員會選舉機制的抗操控能力」。

\subsection{委員會選舉機制的設計空間}
\label{sec:committee-selection}

委員會選舉機制決定了哪些節點將被選入委員會，其設計直接影響系統的安全性與公平性。現有方案大致可分為四種取向：隨機選擇、權益導向、聲譽導向與貢獻導向，各有其優勢與潛在風險。

隨機選擇機制透過密碼學隨機數決定委員會組成，其核心優勢在於不可預測性：攻擊者無法提前知曉哪些節點將被選中，因而難以針對性地部署攻擊。RapidChain \cite{zamani2018rapidchain} 採用分散式隨機數生成協議，確保選舉結果對所有參與者而言都是不可預測且可驗證的。然而，純粹的隨機選擇可能將惡意或低品質的節點選入委員會，且無法反映節點過去的行為表現。權益導向機制將選中機率與節點持有的權益（stake）掛鉤，持有越多權益的節點越可能被選入委員會。這種設計的理論基礎是經濟激勵對齊：高權益節點若行為惡意將面臨更大的經濟損失，因此傾向誠實。以太坊 2.0 的驗證者選舉即採用此機制 \cite{gasper}。然而，權益導向可能導致「富者愈富」的中心化傾向，且無法防範願意承受經濟損失的攻擊者。

聲譽導向機制根據節點的歷史行為表現運算聲譽分數，高聲譽者優先被選入委員會。BESIFL \cite{chen2021robust} 追蹤各節點提交更新的品質，將聲譽作為委員會選舉的權重。此機制能有效過濾曾有惡意行為記錄的節點，但也面臨兩項挑戰：新加入者缺乏歷史記錄，可能陷入「冷啟動」困境；更重要的是，策略性攻擊者可透過長期的誠實行為累積聲譽，待時機成熟後再發動攻擊。貢獻導向機制以節點對聯邦學習的實質貢獻（如訓練資料量、模型品質）作為選舉依據。FLCoin \cite{ren2024scalable} 的滑動視窗機制即屬此類：節點透過提交有效的模型更新獲得「份額」，在視窗內持有份額的節點組成委員會。這種設計與聯邦學習的目標直接對齊，但貢獻指標可能被博弈；例如，攻擊者可先提交高品質更新以獲取委員會席位，再利用此席位通過惡意提案。

實際系統通常結合多種機制以平衡各方考量。BlockDFL \cite{qin2024blockdfl} 採用「權益加權的確定性隨機選擇」：選舉結果由前一區塊雜湊決定（確定性），但各節點被選中的機率與其權益成正比（權益加權）。這種混合設計試圖兼顧不可預測性與經濟激勵，但也繼承了權益導向機制的潛在風險，亦即攻擊者可能透過累積權益逐步提高其影響力。

\subsection{委員會規模與安全性的權衡分析}
\label{sec:committee-size-security}

委員會規模的選擇涉及效率與安全性之間的核心權衡，較小的委員會帶來更低的通訊成本與更快的共識速度，但也更容易被攻擊者滲透，而較大的委員會提供更強的安全保證，卻犧牲了效率優勢。理解這一權衡需要從機率論的角度分析委員會被攻破的風險，而超幾何分佈為此提供了精確的數學工具。當從 $N$ 個節點（其中 $fN$ 個為惡意節點）中隨機選取 $C$ 個組成委員會時，由於委員會成員的選擇是一個無放回抽樣過程，委員會中恰有 $k$ 個惡意節點的機率遵循超幾何分佈，其機率質量函數可表示為：
\begin{equation}
P(X = k) = \frac{\binom{fN}{k} \binom{(1-f)N}{C-k}}{\binom{N}{C}}
\label{eq:hypergeometric}
\end{equation}
其中 $\binom{n}{m}$ 表示二項式係數，代表從 $n$ 個元素中選擇 $m$ 個元素的方式數量。這個公式的分子部分運算了選擇 $k$ 個惡意節點和 $C-k$ 個誠實節點的所有可能組合方式，而分母則是從 $N$ 個節點中選擇 $C$ 個節點的總組合數。

對於採用 PBFT 共識的委員會而言，安全性分析需要區分兩種不同的威脅閾值。第一種閾值為三分之一，當惡意節點超過委員會的三分之一時，攻擊者能夠阻止委員會達成任何共識，因為 PBFT 協議要求至少 $2f+1$ 個節點同意才能通過提案，這種攻擊形式本質上是一種拒絕服務攻擊，雖然無法注入惡意內容，但能夠癱瘓系統的正常運作。第二種閾值為三分之二，這是更為嚴重的威脅情境，當惡意節點佔據委員會超過三分之二的席位時，攻擊者不僅能夠阻止誠實提案通過，更能夠強制通過惡意提案，完全控制委員會的決策結果。在本研究所關注的漸進式委員會佔領攻擊情境中，攻擊者的目標正是達成後者，透過控制委員會來通過有利於自身的提案並排除誠實節點的更新，因此後續的風險分析將以三分之二作為委員會被惡意控制的臨界閾值。

基於上述分析，委員會被惡意控制的風險機率 $P_{mal}$ 可以表示為惡意節點數量達到或超過 $\lfloor 2C/3 \rfloor + 1$ 的累積機率：
\begin{equation}
P_{mal} = P(X \ge \lfloor 2C/3 \rfloor + 1) = \sum_{k=\lfloor 2C/3 \rfloor + 1}^{C} \frac{\binom{fN}{k} \binom{(1-f)N}{C-k}}{\binom{N}{C}}
\label{eq:committee-capture-prob}
\end{equation}
為了具體理解這個機率模型的實際意涵，以下考察一個具代表性的數值案例。假設驗證者總池規模 $N = 100$，網路中惡意節點的比例 $f = 0.3$，即存在 30 個惡意節點和 70 個誠實節點，這是一個相對極端的假設，因為 30\% 的惡意比例已經接近大多數拜占庭容錯系統所能容忍的上限。在這種條件下，不同委員會規模所對應的被惡意控制風險呈現出明顯的差異。當委員會規模 $C=5$ 時，惡意節點需要至少佔據 4 個席位才能達到控制閾值，透過超幾何分佈的運算，這種情況發生的機率約為 2.74\%。當委員會規模增加到 $C=7$ 時，惡意節點需要至少 5 個席位才能控制，此時風險機率約為 2.42\%，略有下降但改善不明顯。

當委員會規模進一步增加到 $C=9$ 時，情況出現了顯著變化，此時惡意節點需要佔據至少 7 個席位才能達到三分之二的控制閾值，而這種情況發生的機率驟降至約 0.28\%，這個數值已經低於許多實際系統所設定的風險容忍度。繼續增加委員會規模，當 $C=11$ 時風險進一步降至約 0.25\%，當 $C=13$ 時則降至約 0.21\%。這些資料揭示了兩個重要的洞察。其一，委員會規模與安全性之間並非線性關係，存在一個「甜蜜點」區間（約 $C=9$ 至 $C=13$），在此區間內增加委員會規模能夠帶來顯著的安全性提升。其二，即使在相當高的惡意節點比例下，只需要一個規模適中的委員會就能將被惡意控制的風險壓制到相當低的水平。然而，委員會規模的增加直接推高了共識的通訊成本，由於 PBFT 的通訊複雜度為 $O(C^2)$，$C=13$ 的委員會其內部通訊量約為 $C=5$ 的 6.76 倍，這種成本增長在頻繁迭代的聯邦學習場景中尤為顯著。

上述分析揭示了委員會架構在傳統設計典範下面臨的根本性困境：在「門檻安全性」的框架內，系統設計者被迫在效率與安全性之間做出取捨，無法同時最大化兩者。然而，這種困境的存在源於一個隱含的假設，即安全性的保障必須依賴於「降低委員會被攻破的機率」。若能改變安全性的實現方式本身，使其不再受制於機率運算，則委員會規模與安全性之間的強耦合關係便有望被解構。這一觀察為第\ref{chap:framework} 章所提出的審計驅動型委員會 BlockDFL提供了理論切入點。

\subsection{基準系統模型：BlockDFL 委員會架構}
\label{sec:blockdfl-baseline}

為深入分析區塊鏈聯邦學習中委員會機制的安全性，本研究採用 BlockDFL \cite{qin2024blockdfl} 作為基準系統模型。BlockDFL 於 2024 年發表於 WWW 會議，代表當前完全去中心化點對點聯邦學習架構的最新進展。該系統透過角色分離、權益加權選舉與拜占庭容錯共識的結合，在效率與安全性之間取得了當前文獻中的最佳平衡。本節將詳細介紹其系統架構、運作流程與獎勵機制，作為後續威脅分析的基礎框架。

\subsubsection{系統角色與職責定義}

BlockDFL 採用角色分離的設計理念，將參與者依據其在每輪訓練中承擔的職責劃分為三種角色：更新提供者、聚合者與驗證者。這種分工模式源於一個核心洞察，在去中心化環境中，若由單一節點同時負責訓練、聚合與驗證，將難以建立有效的制衡機制。透過將這三項職責分派給不同的參與者群體，系統得以在各環節引入相互監督，降低單點惡意行為對全域模型的影響。更新提供者構成系統中的多數參與者，其職責是利用本地私有資料執行模型訓練，並將訓練所得的模型更新提交給聚合者。由於訓練資料始終保留在本地裝置，更新提供者的隱私得以保護，這體現了聯邦學習「資料不動、模型動」的核心價值。

聚合者的職責則是收集來自多個更新提供者的本地更新，執行篩選與聚合運算，將結果打包為全域更新提案並提交給驗證者。每輪訓練中可能有多個聚合者同時運作，各自獨立收集更新並提交競爭性的提案，這種設計避免了單一聚合者壟斷聚合權的風險。驗證者組成委員會，負責評估各聚合者提交的提案品質，並透過拜占庭容錯共識機制選出最佳提案寫入區塊鏈。驗證者的數量通常遠小於更新提供者，以維持共識效率。角色的分配並非固定不變，而是在每輪訓練開始時根據上一區塊的雜湊值與各參與者的權益重新決定。具體而言，區塊雜湊被映射至一個雜湊環，每個參與者依據其權益大小佔據環上相應比例的空間。系統依序從雜湊環上選出聚合者與驗證者，未被選中者則成為更新提供者。

這種機制確保了角色分配的確定性與不可預測性，給定相同的區塊雜湊與權益分布，角色分配結果完全確定，便於驗證；另一方面，由於區塊雜湊在區塊產生前無法預知，參與者難以提前操控自身角色。更重要的是，權益加權的設計使得持有較多權益的參與者更有可能被選為聚合者或驗證者，這反映了系統對「高權益者傾向誠實」的信任假設。值得注意的是，權益不僅影響角色分配的機率，更在聚合過程中扮演關鍵角色。當聚合者收集更新時，來自高權益節點的更新不僅有更高機率被納入聚合候選集，在最終的加權聚合中也被賦予更高的權重，這意味著高權益節點對全域模型演化方向的影響力顯著大於低權益節點。這種設計強化了權益與影響力之間的正相關關係，也為後續討論的權益累積攻擊提供了經濟基礎。

\subsubsection{訓練輪次的運作流程}

BlockDFL 的每輪訓練遵循一個結構化的流程，從角色分配開始，經由本地訓練、聚合、驗證與共識，最終完成全域模型更新與獎勵分配。訓練輪次始於角色分配階段，當新一輪開始時，所有參與者根據最新區塊的雜湊值與當前權益分布，確定性地運算出本輪的角色分配結果。由於運算過程僅依賴公開可驗證的資訊，任何參與者皆可獨立驗證角色分配的正確性，無需依賴中央協調者。角色確定後，更新提供者隨即進入本地訓練階段，在各自的私有資料集上執行若干輪隨機梯度下降，產生本地模型更新並將其發送給聚合者。這些更新僅包含模型參數的變化量，而非原始訓練資料，從而在協作學習與隱私保護之間取得平衡。

聚合階段是 BlockDFL 架構的關鍵環節，其設計直接影響了系統對惡意更新的抵抗能力以及權益機制的運作方式。每個聚合者獨立收集來自更新提供者的本地更新，當收集數量達到預設閾值後，開始執行篩選與聚合程序。篩選的目的在於過濾潛在的惡意更新，其過程分為兩個步驟：首先，聚合者依據更新提供者的權益進行加權隨機抽樣，權益較高的節點有更高機率被納入聚合候選集，這種設計基於「高權益節點更傾向誠實」的假設，同時也創造了一個正反饋機制，使得權益較高的節點更容易影響模型演化；其次，透過本地推論測試評估各候選更新的品質，排除表現異常者。通過篩選的更新隨後進行權益加權聚合，即各更新在聚合過程中的權重正比於其提供者的權益佔比，這種設計使得高權益節點對全域模型的影響力進一步放大。聚合完成後，聚合者將結果打包為全域更新提案，簽署後提交給驗證者委員會。

值得注意的是，由於多個聚合者同時運作且各自獨立收集更新，同一輪中將產生多個競爭性的提案，這為後續的驗證階段提供了選擇空間。驗證與共識階段決定了哪個提案將被接受並寫入區塊鏈。當驗證者委員會收到足夠數量的提案後，驗證程序啟動。每位驗證者獨立使用 Krum 演算法 \cite{blanchard2017machine} 對所有提案進行評分，Krum 分數較低者代表與其他提案的整體距離較小，被視為品質較高。基於評分結果，驗證者對各提案進行投票，僅當某提案的 Krum 分數優於三分之二以上的其他提案時，驗證者才會投下贊成票。投票過程遵循簡化的 PBFT 協議，當某提案獲得超過三分之二驗證者的贊成票時，該提案被正式接受。委員會的領導者隨即將接受的提案連同相關資訊打包成新區塊，廣播至全網。所有參與者收到新區塊後，依據其中的全域更新同步更新本地模型，完成本輪訓練。

\subsubsection{獎勵機制與激勵設計}

BlockDFL 的獎勵機制遵循「有貢獻才有回報」的設計原則，旨在解決分散式系統中普遍存在的搭便車問題。在傳統聯邦學習中，無論參與者是否真正貢獻高品質的訓練成果，皆可獲得最終全域模型的使用權，這削弱了誠實參與的激勵。BlockDFL 透過將權益獎勵與「被選中」直接綁定，確保只有對本輪全域模型更新有實質貢獻的參與者才能獲得回報，從而建立起正向的激勵結構。具體而言，當某一提案通過委員會共識並被寫入區塊鏈時，系統將權益增量分配給三類參與者。第一類是提交該提案的聚合者，其承擔了收集更新、執行篩選與聚合運算的工作，並承受提案可能未被選中的風險。第二類是本地更新被納入該提案的更新提供者，他們貢獻了訓練運算資源與本地資料的價值。第三類是對該提案投下贊成票的驗證者，他們執行了驗證運算並參與了共識決策。這三類參與者均分本輪的權益獎勵，其身份被明確記錄於區塊之中，確保獎勵分配的透明與可驗證。

相對地，未對本輪全域模型更新做出貢獻的參與者則不獲得任何獎勵。這包括提案未被選中的其他聚合者、本地更新未被納入獲選提案的更新提供者、以及對獲選提案投下反對票或未參與投票的驗證者。這種設計創造了明確的激勵導向，聚合者有動機提交高品質的提案以提高被選中的機率，更新提供者有動機提交優質的本地更新以增加被納入提案的可能性，驗證者則有動機投票支持真正優質的提案，因為只有投票與最終結果一致時才能獲得獎勵。然而，此獎勵機制在激勵誠實行為的同時，也創造了一個具有正反饋特性的動態系統。當參與者獲得權益獎勵時，其總權益增加，這直接提升了該參與者在未來輪次被選為聚合者或驗證者的機率，進而增加其獲得更多獎勵的機會。

這種正反饋循環透過前述的權益加權機制進一步強化，高權益節點不僅更容易被選為關鍵角色，其作為更新提供者時提交的更新也更容易被納入聚合，並在聚合過程中獲得更高權重。BlockDFL 的設計者預期此正反饋將使持續誠實貢獻的參與者逐漸累積優勢，而惡意參與者的影響力則相對削弱 \cite{qin2024blockdfl}。這項預期建立在一個關鍵假設之上，獲得高權益的參與者必然是長期誠實貢獻者。然而，若攻擊者能夠在潛伏期間偽裝成誠實參與者並成功累積權益，此正反饋機制反而可能成為攻擊者鞏固優勢的工具。更值得關注的是，權益在聚合過程中的雙重作用，既影響被選中機率又決定聚合權重，使得高權益攻擊者不僅能更頻繁地參與決策，更能在參與時施加更大的影響力，這種複合效應顯著放大了權益累積對系統安全性的潛在威脅。

\subsubsection{本研究採用此模型的理由}

本研究選擇 BlockDFL 作為基準系統模型，基於以下四項考量。首先，BlockDFL 代表了區塊鏈聯邦學習委員會架構的最新技術水準，其於 2024 年發表於頂級網路研討會，融合了角色分離、權益加權選舉、雙層評分機制與拜占庭容錯共識等多項先進設計，具有高度的代表性。其次，BlockDFL 的系統定義清晰完整，論文詳細說明了角色職責、運作流程與獎勵規則，這為形式化的威脅分析提供了堅實基礎。相較於部分僅提供概念性描述的研究，BlockDFL 的明確定義使得安全性分析能夠建立在具體的系統行為之上，而非抽象的假設。第三，BlockDFL 的委員會機制與獎勵設計具有廣泛的通用性。儘管不同 BCFL 系統在具體實現上存在差異，但「小型委員會執行共識」與「權益驅動的角色選舉」已成為此領域的主流設計典範。因此，針對 BlockDFL 所發現的安全問題與提出的防禦機制，在原理上可推廣至採用類似架構的其他系統。最後，BlockDFL 已有公開的實驗資料與效能基準，這為本研究後續的防禦機制評估提供了可比較的參照點。綜合以上考量，BlockDFL 是本研究進行威脅建模與防禦設計的理想分析對象。

\section{現有驗證方法與其局限}
\label{sec:verification-limitations}

\subsection{密碼學驗證方法的運算瓶頸}
\label{sec:zkml-limitations}

零知識機器學習（Zero-Knowledge Machine Learning, zkML）代表了密碼學驗證方法在機器學習領域的前沿探索 \cite{chen2024zkml}。其核心理念是將機器學習運算轉換為算術電路，並生成零知識證明，使驗證者無需重新執行運算即可確認結果的正確性。這種方法在理論上提供了最強的安全保證：證明的正確性完全依賴密碼學假設，無需信任任何參與方。若能將 zkML 應用於聯邦學習的聚合驗證，系統將能在不揭露個別更新內容的前提下，證明聚合結果確實是由指定的本地更新按照預定規則運算而得。

然而，zkML 面臨嚴峻的運算效能挑戰，這使其在當前技術條件下難以應用於實際的聯邦學習系統。將神經網路運算轉換為算術電路的過程會產生大量的多項式約束，約束數量隨模型複雜度急劇膨脹。根據現有基準測試，即使是相對簡單的 LeNet \cite{lecun1998gradient} 模型，其約束數量也可達數億級別；對於 ResNet-18 \cite{he2016deep} 等級的模型，證明生成時間需要近一分鐘；而對於 VGG16 \cite{simonyan2015very} 或更大規模的模型，證明生成可能耗時數十分鐘甚至數小時，且需要數百 GB 乃至 TB 級別的記憶體 \cite{chen2024zkml}。考慮到聯邦學習通常需要數百至數千輪迭代，每輪都執行如此耗時的證明生成顯然不切實際。

更關鍵的局限在於 zkML 難以支援拜占庭容錯聚合演算法。Krum \cite{blanchard2017machine} 與 Multi-Krum 等防禦性聚合方法需要運算所有客戶端更新之間的成對距離，這在零知識電路中會產生 $O(n^2 \cdot d)$ 的約束爆炸，其中 $n$ 為客戶端數量，$d$ 為模型參數維度。排序與中位數運算在算術電路中同樣極度昂貴。現有的 zkFL 方案如 RiseFL \cite{zhu2024risefl} 僅能支援 L2-norm 有效性檢查等簡單驗證，而無法實現完整的拜占庭容錯聚合驗證。這意味著即使克服了效能瓶頸，zkML 仍無法為採用 Krum 等防禦機制的系統（如 BlockDFL）提供聚合正確性的密碼學證明。

\subsection{樂觀執行方法的架構限制}
\label{sec:opml-limitations}

樂觀機器學習（Optimistic Machine Learning, opML）採用與 zkML 截然不同的設計哲學：預設所有運算結果都是正確的，僅在有參與者提出質疑時才啟動驗證程序 \cite{conway2024opml}。這種「樂觀執行」的模式大幅降低了正常情況下的運算成本，因為絕大多數時候驗證程序不會被觸發。當爭議發生時，系統透過互動式的二分協議（Bisection Protocol）逐步縮小爭議範圍，最終定位至單一運算步驟，由鏈上的欺詐證明虛擬機（Fraud Proof Virtual Machine, FPVM）進行仲裁。ORA Protocol 已展示此方法可支援 LLaMA 2 \cite{touvron2023llama} 等數十億參數規模的模型在以太坊上運行 \cite{ora2024opml}。

然而，opML 的架構設計與聯邦學習的需求存在根本性的衝突。首先是挑戰期的問題。為確保驗證者有充足時間偵測並提交欺詐證明，主流的樂觀執行系統如 Optimism \cite{optimism2024rollup} 與 Arbitrum \cite{kalodner2018arbitrum} 採用長達一週的挑戰期。這種設計對於區塊鏈交易的最終確認或許可以接受，但對於需要快速迭代的聯邦學習訓練而言則完全不可行。若每輪聚合都需等待一週才能確認，數百輪的訓練將耗時數年。即使大幅縮短挑戰期，仍會顯著拖慢訓練進度，且可能因驗證者反應時間不足而削弱安全保證。

其次，opML 的信任模型與 BCFL 的多驗證者場景存在落差。opML 建立在「AnyTrust」假設之上：只要存在至少一個誠實的驗證者願意監控並挑戰錯誤結果，系統就是安全的。這本質上是一個單一提交者與單一挑戰者之間互相對抗的兩方爭議模型。然而，BCFL 的委員會共識涉及多個驗證者對多個提案的集體決策，這種多方參與的結構難以直接套用 opML 的爭議解決框架。此外，opML 的設計假設運算輸入（如模型更新）是公開可見的，以便驗證者能夠重新執行運算並發現錯誤。這與聯邦學習對更新隱私的保護需求存在張力。

\subsection{委員會驗證方法的安全假設}
\label{sec:committee-verification}

相較於密碼學方法與樂觀執行方法，基於委員會共識的驗證方法在效率與實用性之間取得了較佳的平衡，因而成為當前 BCFL 系統的主流選擇。這類方法的核心思想是由一個小型委員會代全網執行驗證與共識，透過拜占庭容錯協議確保只要委員會中的誠實成員佔據多數，驗證結果就是可信的。前文介紹的 BlockDFL 即屬此類，其他代表性系統包括 FLCoin \cite{ren2024scalable} 與 BFLC \cite{li2021blockchain}。

FLCoin 提出基於滑動視窗的動態委員會機制，將聯邦學習的貢獻歷史作為委員會成員資格的依據。節點透過提交有效的模型更新獲得「份額」，在固定大小的滑動視窗內持有份額的節點組成當輪委員會。這種設計使委員會組成與 FL 目標直接對齊，且透過視窗的滑動實現成員的動態更替。FLCoin 的安全性分析顯示，在全網惡意節點比例不超過 25\% 且視窗大小為 100 的條件下，委員會安全的機率可達 98.4\% \cite{ren2024scalable}。然而，論文並未深入分析惡意節點透過策略性參與逐步累積份額的可能性，其實驗也假設無惡意節點參與，未驗證對抗性累積策略的防禦效果。

BFLC 開創性地將委員會共識引入 BCFL，採用聲譽機制決定委員會組成 \cite{li2021blockchain}。系統追蹤各節點提交更新的品質歷史，高聲譽者優先被選入委員會。委員會成員使用 K-fold 交叉驗證評估提交的模型更新，透過共識決定是否接受。這種設計能有效過濾曾有不良記錄的節點，但也面臨冷啟動問題：新加入者缺乏歷史記錄，難以建立初始信任。更重要的是，後續研究明確指出 BFLC「容易被惡意節點混入委員會，從而導致系統偏差」\cite{qin2024blockdfl}，當惡意節點佔據委員會半數席位時即可發動攻擊。

綜觀這些委員會驗證方法，它們共享一個根本性的安全假設：委員會的誠實多數。無論採用隨機抽樣、權益加權、聲譽評分或貢獻歷史等任何選舉機制，所有方案都假設某種形式的誠實多數能夠在委員會層級得到維持。這一假設在面對靜態的、比例固定的惡意節點時或許成立，但當攻擊者採取動態的、策略性的行為時，其有效性便值得商榷。下一小節將進一步分析這種靜態假設的盲區。

\subsection{系統性局限：靜態分析的盲區}
\label{sec:static-analysis-blindspot}

回顧上述三類驗證方法，可以發現一個貫穿其中的系統性局限：現有的安全性分析幾乎都建立在「攻擊者資源固定」的靜態假設之上。zkML 的安全性證明假設攻擊者的運算能力無法突破密碼學難題；opML 假設至少存在一個持續監控的誠實驗證者；委員會方法則假設惡意節點在全網中的比例 $\alpha$ 是一個固定的常數，並據此運算委員會被攻破的機率。這種「快照式」的分析視角忽略了一個關鍵的動態因素：理性的攻擊者會根據系統狀態調整其策略，而非機械地執行固定行為。

以委員會方法為例，第 \ref{sec:committee-size-security} 節的超幾何分佈分析假設每輪選舉都是從固定的惡意節點比例中獨立抽樣。然而，若系統的獎勵機制存在正反饋特性（例如在 BlockDFL 架構中，獲得獎勵的節點將累積更多權益，進而提高未來被選中的機率），則各輪選舉之間並非獨立事件。攻擊者可利用此特性採取「耐心策略」：在初期表現完全誠實以累積權益與聲譽，僅在其控制的節點佔據委員會優勢時才發動攻擊。這種行為模式無法被靜態的機率分析所捕捉。

更根本的問題在於，現有分析未區分「惡意節點的存在比例」與「惡意節點的有效影響力」。在權益加權的選舉機制中，一個持有 10\% 權益的惡意節點，其對委員會組成的影響力可能遠大於十個各持有 1\% 權益的惡意節點。若攻擊者能夠透過合法途徑（累積獎勵、建立聲譽）集中權益於少數節點，即使其控制的節點數量佔全網比例不高，仍可能在委員會層級取得超額的影響力。FLCoin 的分析雖然考慮了惡意節點比例與委員會安全機率的關係，但未分析惡意節點比例本身可能隨時間演變的動態過程。BlockDFL 假設「持有大量權益的參與者傾向誠實」，但未充分論證此假設在面對長期潛伏的策略性攻擊者時是否仍然成立。

表 \ref{tab:verification-comparison} 總結了三類驗證方法在安全性、效率與通用性三個維度上的特性。可以看出，沒有任何一種方法能夠同時滿足所有需求：zkML 提供最強的安全保證但效率過低且無法支援複雜聚合；opML 效率較高但挑戰期過長且架構不適用於多驗證者場景；委員會方法在效率與實用性上表現最佳，但其安全性依賴可能被違反的誠實多數假設。這種「安全性-效率-通用性」的三難困境，構成了本領域研究的核心挑戰。

\begin{table}[htbp]
\centering
\caption{BCFL 驗證方法比較}
\label{tab:verification-comparison}
\begin{tabularx}{\textwidth}{|l|X|X|X|}
\hline
\textbf{方法類別} & \textbf{安全性基礎} & \textbf{主要效率瓶頸} & \textbf{對 BCFL 的適用性} \\ \hline
zkML & 密碼學證明，無需信任假設 & 證明生成耗時數分鐘至數小時 & 低：無法支援 Krum 等複雜聚合 \\ \hline
opML & 經濟激勵與 AnyTrust 假設 & 挑戰期長達數天 & 低：架構不適用於多驗證者場景 \\ \hline
委員會共識 & 委員會誠實多數假設 & 共識通訊成本 $O(c^2)$ & 高：主流選擇，但假設可能被違反 \\ \hline
\end{tabularx}
\end{table}

\section{研究缺口：從激勵設計到安全隱患}
\label{sec:research-gap}

前述分析揭示了現有 BCFL 驗證方法的共同盲區：靜態的安全性假設無法應對動態的攻擊策略。本節將聚焦於當前最具實用性的委員會驗證方法，深入探討其獎勵機制如何在激勵誠實行為的同時，也為策略性攻擊者創造了可利用的漏洞。這一分析將指向本研究欲填補的關鍵缺口，並為第三章的威脅模型建構奠定基礎。

\subsection{獎勵機制的正反饋特性}
\label{sec:reward-feedback}

第 \ref{sec:blockdfl-baseline} 節詳細介紹了 BlockDFL 的獎勵機制：當某一提案通過委員會共識時，提交該提案的聚合者、更新被納入的更新提供者、以及投贊成票的驗證者共同分享權益獎勵。這種設計的初衷是解決搭便車問題，確保只有實質貢獻者才能獲得回報。然而，從系統動態的角度審視，此機制創造了一個具有正反饋特性的循環結構。

正反饋的運作邏輯可描述如下：節點獲得權益獎勵後，其總權益增加；由於角色選舉採用權益加權機制，權益增加直接提升該節點在未來輪次被選為聚合者或驗證者的機率；作為聚合者或驗證者，節點更有機會參與被接受的提案，從而獲得更多獎勵。這種「獎勵 → 權益增加 → 選中機率提升 → 更多獎勵」的循環，在數學上構成一個正反饋系統。BlockDFL 的設計者預期此特性將使誠實參與者逐漸累積優勢，因為他們持續提交高品質更新並誠實驗證，從而獲得穩定的獎勵流入。相對地，惡意參與者若因提交低品質更新或投票與最終結果不一致而錯失獎勵，其相對權益份額將逐漸稀釋。

然而，這一預期建立在一個隱含假設之上：系統能夠有效區分誠實行為與惡意行為，並據此差異化地分配獎勵。問題在於，當惡意節點選擇「偽裝誠實」，意即在攻擊發動前完全模仿誠實節點的行為，系統便無法將其與真正的誠實節點區分開來。在這種情況下，惡意節點同樣能夠獲得獎勵、累積權益、提升影響力，正反饋機制反而成為攻擊者滲透系統的工具。

\subsection{策略性攻擊者的潛在威脅}
\label{sec:strategic-attacker}

基於上述分析，可以設想一種策略性的攻擊模式：攻擊者控制的節點在初期階段完全表現為誠實參與者，正常參與訓練、提交高品質更新、在驗證時投票支持真正優質的提案。透過這種「潛伏」行為，攻擊節點逐步累積權益與聲譽，提高其被選為聚合者或驗證者的機率。當攻擊者評估其控制的節點已在委員會中佔據足夠優勢時（例如在某一輪選舉中恰好佔據超過三分之一的驗證者席位），才發動實際攻擊。

這種攻擊模式的危險性在於其隱蔽性與自我強化性。在潛伏階段，攻擊節點的行為與誠實節點完全一致，現有的異常偵測機制無從識別。更重要的是，一旦攻擊者在某一輪成功控制委員會，他們可以操控共識結果，將獎勵僅分配給自己控制的節點（透過接受包含攻擊節點更新的提案、拒絕僅包含誠實節點更新的提案），同時確保攻擊節點作為「投贊成票的驗證者」獲得驗證獎勵。這種操控使攻擊者的權益份額在成功攻擊後加速增長，進一步提高其在未來輪次控制委員會的機率，形成惡性循環。

現有文獻已零星地意識到委員會滲透的風險。FedBlock 在其未來展望中指出：「目前版本中，智能合約以隨機方式選取客戶端作為驗證者，但此選擇標準可能並非最佳」，並警告「驗證者本身亦可能是惡意的」\cite{nguyen2024fedblock}。BFLC 的後續評論指出該系統「容易被惡意節點混入委員會」\cite{qin2024blockdfl}。然而，這些觀察多停留在定性描述的層次，尚未發展出系統性的威脅模型來形式化分析此類攻擊的具體機制、成功條件與潛在危害。特別是，現有研究未充分探討獎勵機制的正反饋特性如何與委員會選舉機制交互作用，使得攻擊者能夠透過「合法」途徑逐步擴大影響力。

\subsection{本研究的切入點}
\label{sec:research-contribution}

綜合本章的分析，可以明確界定當前 BCFL 委員會安全性研究的核心缺口：

第一，\textbf{缺乏針對策略性攻擊者的形式化威脅模型}。現有安全性分析假設惡意節點比例固定且行為模式單一，未考慮攻擊者可能採取的動態策略，如長期潛伏、選擇性攻擊時機、以及利用正反饋機制累積優勢。

第二，\textbf{缺乏對獎勵機制安全性意涵的深入分析}。現有研究將獎勵機制視為激勵誠實行為的工具，但未系統性地審視其正反饋特性可能被攻擊者利用的風險。

第三，\textbf{缺乏能夠偵測與抵禦漸進式滲透的防禦機制}。無論是隨機選擇、權益加權或聲譽導向，現有委員會選舉機制都未針對「透過合法途徑累積影響力」的攻擊模式設計相應的防護措施。

針對上述缺口，本研究將在後續章節展開以下工作。第三章將形式化定義「漸進式委員會佔領攻擊」（Progressive Committee Capture Attack, PCCA）的威脅模型，基於第 \ref{sec:blockdfl-baseline} 節建立的 BlockDFL 系統模型，分析攻擊者如何利用獎勵機制的正反饋特性逐步滲透委員會，並量化攻擊在不同參數設定下的成功機率與所需時間。第四章將提出針對 PCCA 的防禦架構設計，透過打破正反饋循環與引入動態挑戰機制，在維持系統效率的前提下提升對策略性攻擊者的抵抗能力。第五章將透過實驗評估所提出方法的有效性，驗證其在多種攻擊場景下的防護效能。

\end{ZhChapter}